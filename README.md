# ğŸ§  Convolutional Neural Network for MNIST Digit Classification

### ğŸ“˜ Overview

This project implements a **Convolutional Neural Network (CNN)** to classify handwritten digits from the **MNIST dataset**.
The MNIST dataset contains 70,000 grayscale images (28Ã—28 pixels each) of digits (0â€“9).
The model learns to recognise these digits through supervised learning and outputs classification probabilities for each class.

---

## ğŸ¯ Objectives

* Build and train a CNN using TensorFlow/Keras to recognise handwritten digits.
* Demonstrate core deep learning concepts such as convolution, pooling, and activation functions.
* Evaluate accuracy and visualize model performance.
* Provide a clear, modular project structure that can be easily extended.

---

## ğŸ§© Neural Network Architecture

| Layer                         | Output Shape | Parameters | Description                                         |
| ----------------------------- | ------------ | ---------- | --------------------------------------------------- |
| **Conv2D (32 filters, 3Ã—3)**  | 26Ã—26Ã—32     | 320        | Extracts local features such as edges and textures. |
| **MaxPooling2D (2Ã—2)**        | 13Ã—13Ã—32     | 0          | Reduces spatial dimensions and computation.         |
| **Conv2D (64 filters, 3Ã—3)**  | 11Ã—11Ã—64     | 18,496     | Learns more complex visual patterns.                |
| **MaxPooling2D (2Ã—2)**        | 5Ã—5Ã—64       | 0          | Further compresses the feature maps.                |
| **Flatten**                   | 1600         | 0          | Converts the 2D matrix into a 1D vector.            |
| **Dense (128 units, ReLU)**   | 128          | 204,928    | Fully connected layer for high-level reasoning.     |
| **Dense (10 units, Softmax)** | 10           | 1,290      | Output layer for digit classification (0â€“9).        |

**Optimizer:** Adam
**Loss Function:** Categorical Crossentropy
**Metrics:** Accuracy

---

## ğŸ“Š Model Performance

| Metric                  | Result   |
| ----------------------- | -------- |
| **Training Accuracy**   | â‰ˆ 99%    |
| **Validation Accuracy** | â‰ˆ 99%    |
| **Test Accuracy**       | â‰ˆ 98â€“99% |

Visual outputs include confusion matrices, sample predictions, and accuracy/loss plots.

---

## âš™ï¸ Project Structure

```
CNN_MNIST_Project/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data_out_pt1.npz         # Training and Test Data
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ CNN_mnist.ipynb
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Project overview and usage instructions
```

This modular design keeps your code organized, maintainable, and easy to extend.

---

## ğŸ§° Requirements

To install dependencies locally:

```bash
pip install -r requirements.txt
```

**Example `requirements.txt`:**

```
tensorflow
numpy
matplotlib
pandas
```

If youâ€™re running the notebook in **Google Colab**, these libraries are already preinstalled.

---

## â–¶ï¸ Running the Project in Google Colab

1. Open [Google Colab](https://colab.research.google.com/).
2. Upload the `notebooks/CNN_mnist.ipynb` file.
3. Ensure GPU acceleration is enabled:

   * Go to **Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU**.
4. Run all cells sequentially (`Shift + Enter`).

The notebook will:

* Load and preprocess the MNIST dataset.
* Build and train the CNN model.
* Evaluate test performance.
* Visualize accuracy, loss, and predictions.

---

## ğŸš€ Future Improvements

* Implement **data augmentation** for improved robustness.
* Save and deploy the trained model as a web app or API service.
* Experiment with deeper architectures or transfer learning.

---

## ğŸ§¾ License

This project is provided for **educational and research purposes**.
You may modify or extend it with proper attribution.

---

## ğŸ‘¤ Author

**Developed by:** Michael Di Giantomasso
**LinkedIn:** [linkedin.com/in/michael-di-giantomasso](https://www.linkedin.com/in/michael-di-giantomasso-3b1043243/)
